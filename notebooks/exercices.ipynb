{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Corentin312/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf515a2-8a44-496f-e3b1-5947dca41bb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exam_2025'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 59 (delta 21), reused 20 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (59/59), 1.41 MiB | 16.56 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 312"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On peut utiliser de la regression linéaire\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Préparer les données\n",
        "X_train = train_set['inputs']  # Matrice des prédicteurs (x, y, z) pour l'entraînement\n",
        "y_train = train_set['targets']  # Vecteur des cibles (t) pour l'entraînement\n",
        "X_test = test_set['inputs']    # Matrice des prédicteurs pour le test\n",
        "y_test = test_set['targets']    # Vecteur des cibles pour le test\n",
        "\n",
        "# 2. Créer un modèle de régression linéaire\n",
        "model = LinearRegression()\n",
        "\n",
        "# 3. Entraîner le modèle sur les données d'entraînement\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Afficher les coefficients estimés (theta_k)\n",
        "print(\"theta0 :\", model.intercept_)\n",
        "print(\"theta 1 a 3 :\", model.coef_)\n",
        "\n",
        "# 5. Prédire les valeurs pour les données de test\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# 6. Calculer l'erreur quadratique moyenne (MSE)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"Mean Squared Error test :\", mse)"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5e734b-1e5d-408d-e5e7-cc5f7c449c64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta0 : 15.576677853661185\n",
            "theta 1 a 3 : [3.04507485 3.09796146 6.2816341 ]\n",
            "Mean Squared Error test : 4.1952892768798575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Un simple reseau lineaire suffit pour ce cas la car on cherche a minimiser t- θ0 + θ1x + θ2y + θ3z avec une MSE\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# A coder :\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear = nn.Linear(3, 1)  # 3 entrées (x, y, z), 1 sortie (t), la generalisation se fait en changeant le nb 3\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mySimpleNet(batch_inputs.float())\n",
        "\n",
        "        loss = criterion(outputs, batch_targets.float().unsqueeze(1))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        ""
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c0a3d5-4b43-4c4c-f5eb-8353057cc633"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 239.5751\n",
            "Epoch [2/500], Loss: 158.1189\n",
            "Epoch [3/500], Loss: 96.4908\n",
            "Epoch [4/500], Loss: 52.6864\n",
            "Epoch [5/500], Loss: 31.8644\n",
            "Epoch [6/500], Loss: 26.9871\n",
            "Epoch [7/500], Loss: 14.8501\n",
            "Epoch [8/500], Loss: 10.5724\n",
            "Epoch [9/500], Loss: 6.3731\n",
            "Epoch [10/500], Loss: 6.8010\n",
            "Epoch [11/500], Loss: 6.1861\n",
            "Epoch [12/500], Loss: 5.9058\n",
            "Epoch [13/500], Loss: 5.6784\n",
            "Epoch [14/500], Loss: 5.2387\n",
            "Epoch [15/500], Loss: 5.7223\n",
            "Epoch [16/500], Loss: 3.9443\n",
            "Epoch [17/500], Loss: 4.3086\n",
            "Epoch [18/500], Loss: 5.0385\n",
            "Epoch [19/500], Loss: 4.5024\n",
            "Epoch [20/500], Loss: 4.5153\n",
            "Epoch [21/500], Loss: 4.1080\n",
            "Epoch [22/500], Loss: 4.2589\n",
            "Epoch [23/500], Loss: 4.3054\n",
            "Epoch [24/500], Loss: 4.1815\n",
            "Epoch [25/500], Loss: 4.6702\n",
            "Epoch [26/500], Loss: 3.7221\n",
            "Epoch [27/500], Loss: 4.8442\n",
            "Epoch [28/500], Loss: 4.5919\n",
            "Epoch [29/500], Loss: 3.8462\n",
            "Epoch [30/500], Loss: 3.7651\n",
            "Epoch [31/500], Loss: 4.0006\n",
            "Epoch [32/500], Loss: 3.9025\n",
            "Epoch [33/500], Loss: 4.1443\n",
            "Epoch [34/500], Loss: 3.6227\n",
            "Epoch [35/500], Loss: 4.3346\n",
            "Epoch [36/500], Loss: 5.2336\n",
            "Epoch [37/500], Loss: 4.4491\n",
            "Epoch [38/500], Loss: 3.2770\n",
            "Epoch [39/500], Loss: 3.0791\n",
            "Epoch [40/500], Loss: 4.0028\n",
            "Epoch [41/500], Loss: 3.8413\n",
            "Epoch [42/500], Loss: 4.4314\n",
            "Epoch [43/500], Loss: 4.1975\n",
            "Epoch [44/500], Loss: 4.2218\n",
            "Epoch [45/500], Loss: 4.6275\n",
            "Epoch [46/500], Loss: 3.8784\n",
            "Epoch [47/500], Loss: 4.2688\n",
            "Epoch [48/500], Loss: 3.6641\n",
            "Epoch [49/500], Loss: 4.3593\n",
            "Epoch [50/500], Loss: 4.0045\n",
            "Epoch [51/500], Loss: 4.3675\n",
            "Epoch [52/500], Loss: 4.1363\n",
            "Epoch [53/500], Loss: 4.0211\n",
            "Epoch [54/500], Loss: 4.3910\n",
            "Epoch [55/500], Loss: 4.8488\n",
            "Epoch [56/500], Loss: 4.2653\n",
            "Epoch [57/500], Loss: 4.4953\n",
            "Epoch [58/500], Loss: 4.8475\n",
            "Epoch [59/500], Loss: 4.3919\n",
            "Epoch [60/500], Loss: 3.1634\n",
            "Epoch [61/500], Loss: 4.7112\n",
            "Epoch [62/500], Loss: 4.1054\n",
            "Epoch [63/500], Loss: 4.8141\n",
            "Epoch [64/500], Loss: 4.2030\n",
            "Epoch [65/500], Loss: 4.1172\n",
            "Epoch [66/500], Loss: 3.7683\n",
            "Epoch [67/500], Loss: 3.9480\n",
            "Epoch [68/500], Loss: 3.4466\n",
            "Epoch [69/500], Loss: 4.3612\n",
            "Epoch [70/500], Loss: 4.2116\n",
            "Epoch [71/500], Loss: 3.6129\n",
            "Epoch [72/500], Loss: 4.0470\n",
            "Epoch [73/500], Loss: 4.7500\n",
            "Epoch [74/500], Loss: 4.5837\n",
            "Epoch [75/500], Loss: 4.3068\n",
            "Epoch [76/500], Loss: 4.2605\n",
            "Epoch [77/500], Loss: 4.2110\n",
            "Epoch [78/500], Loss: 4.6945\n",
            "Epoch [79/500], Loss: 4.6356\n",
            "Epoch [80/500], Loss: 3.2921\n",
            "Epoch [81/500], Loss: 4.3388\n",
            "Epoch [82/500], Loss: 3.2267\n",
            "Epoch [83/500], Loss: 3.4355\n",
            "Epoch [84/500], Loss: 3.3007\n",
            "Epoch [85/500], Loss: 4.6752\n",
            "Epoch [86/500], Loss: 4.0017\n",
            "Epoch [87/500], Loss: 4.9205\n",
            "Epoch [88/500], Loss: 3.3845\n",
            "Epoch [89/500], Loss: 4.8675\n",
            "Epoch [90/500], Loss: 3.6700\n",
            "Epoch [91/500], Loss: 5.0185\n",
            "Epoch [92/500], Loss: 3.7437\n",
            "Epoch [93/500], Loss: 4.1332\n",
            "Epoch [94/500], Loss: 3.4516\n",
            "Epoch [95/500], Loss: 3.7056\n",
            "Epoch [96/500], Loss: 4.2335\n",
            "Epoch [97/500], Loss: 4.2629\n",
            "Epoch [98/500], Loss: 4.6267\n",
            "Epoch [99/500], Loss: 4.0916\n",
            "Epoch [100/500], Loss: 4.3210\n",
            "Epoch [101/500], Loss: 3.5819\n",
            "Epoch [102/500], Loss: 4.5726\n",
            "Epoch [103/500], Loss: 3.3823\n",
            "Epoch [104/500], Loss: 4.1665\n",
            "Epoch [105/500], Loss: 4.9324\n",
            "Epoch [106/500], Loss: 5.1058\n",
            "Epoch [107/500], Loss: 5.5415\n",
            "Epoch [108/500], Loss: 4.5104\n",
            "Epoch [109/500], Loss: 4.6417\n",
            "Epoch [110/500], Loss: 4.8300\n",
            "Epoch [111/500], Loss: 2.8525\n",
            "Epoch [112/500], Loss: 4.9927\n",
            "Epoch [113/500], Loss: 5.1065\n",
            "Epoch [114/500], Loss: 3.9948\n",
            "Epoch [115/500], Loss: 3.3637\n",
            "Epoch [116/500], Loss: 3.3462\n",
            "Epoch [117/500], Loss: 4.3206\n",
            "Epoch [118/500], Loss: 4.1698\n",
            "Epoch [119/500], Loss: 5.2536\n",
            "Epoch [120/500], Loss: 4.0043\n",
            "Epoch [121/500], Loss: 4.8913\n",
            "Epoch [122/500], Loss: 3.8599\n",
            "Epoch [123/500], Loss: 4.9643\n",
            "Epoch [124/500], Loss: 4.3801\n",
            "Epoch [125/500], Loss: 3.3684\n",
            "Epoch [126/500], Loss: 4.7904\n",
            "Epoch [127/500], Loss: 3.6969\n",
            "Epoch [128/500], Loss: 4.4875\n",
            "Epoch [129/500], Loss: 4.4773\n",
            "Epoch [130/500], Loss: 4.2116\n",
            "Epoch [131/500], Loss: 3.8365\n",
            "Epoch [132/500], Loss: 4.7179\n",
            "Epoch [133/500], Loss: 4.2805\n",
            "Epoch [134/500], Loss: 4.4312\n",
            "Epoch [135/500], Loss: 3.8123\n",
            "Epoch [136/500], Loss: 4.2555\n",
            "Epoch [137/500], Loss: 3.8647\n",
            "Epoch [138/500], Loss: 3.6618\n",
            "Epoch [139/500], Loss: 5.5842\n",
            "Epoch [140/500], Loss: 3.7714\n",
            "Epoch [141/500], Loss: 4.8126\n",
            "Epoch [142/500], Loss: 4.5201\n",
            "Epoch [143/500], Loss: 3.8711\n",
            "Epoch [144/500], Loss: 4.8406\n",
            "Epoch [145/500], Loss: 4.3694\n",
            "Epoch [146/500], Loss: 3.9046\n",
            "Epoch [147/500], Loss: 4.1048\n",
            "Epoch [148/500], Loss: 3.9648\n",
            "Epoch [149/500], Loss: 4.0388\n",
            "Epoch [150/500], Loss: 3.9517\n",
            "Epoch [151/500], Loss: 4.5132\n",
            "Epoch [152/500], Loss: 3.7046\n",
            "Epoch [153/500], Loss: 4.0708\n",
            "Epoch [154/500], Loss: 3.9185\n",
            "Epoch [155/500], Loss: 3.2963\n",
            "Epoch [156/500], Loss: 4.3431\n",
            "Epoch [157/500], Loss: 4.3837\n",
            "Epoch [158/500], Loss: 3.4922\n",
            "Epoch [159/500], Loss: 4.4917\n",
            "Epoch [160/500], Loss: 4.1222\n",
            "Epoch [161/500], Loss: 4.1718\n",
            "Epoch [162/500], Loss: 3.5278\n",
            "Epoch [163/500], Loss: 4.0375\n",
            "Epoch [164/500], Loss: 3.3194\n",
            "Epoch [165/500], Loss: 3.6463\n",
            "Epoch [166/500], Loss: 4.3693\n",
            "Epoch [167/500], Loss: 4.1617\n",
            "Epoch [168/500], Loss: 4.5035\n",
            "Epoch [169/500], Loss: 3.4800\n",
            "Epoch [170/500], Loss: 4.4158\n",
            "Epoch [171/500], Loss: 2.8503\n",
            "Epoch [172/500], Loss: 4.1518\n",
            "Epoch [173/500], Loss: 3.6354\n",
            "Epoch [174/500], Loss: 3.7865\n",
            "Epoch [175/500], Loss: 3.4587\n",
            "Epoch [176/500], Loss: 3.2121\n",
            "Epoch [177/500], Loss: 4.7011\n",
            "Epoch [178/500], Loss: 4.3977\n",
            "Epoch [179/500], Loss: 4.3439\n",
            "Epoch [180/500], Loss: 4.0777\n",
            "Epoch [181/500], Loss: 3.7419\n",
            "Epoch [182/500], Loss: 4.6519\n",
            "Epoch [183/500], Loss: 4.2121\n",
            "Epoch [184/500], Loss: 3.4796\n",
            "Epoch [185/500], Loss: 3.0779\n",
            "Epoch [186/500], Loss: 4.5639\n",
            "Epoch [187/500], Loss: 3.1385\n",
            "Epoch [188/500], Loss: 3.9470\n",
            "Epoch [189/500], Loss: 3.0440\n",
            "Epoch [190/500], Loss: 5.3116\n",
            "Epoch [191/500], Loss: 4.8479\n",
            "Epoch [192/500], Loss: 4.1220\n",
            "Epoch [193/500], Loss: 4.9236\n",
            "Epoch [194/500], Loss: 4.6735\n",
            "Epoch [195/500], Loss: 4.8050\n",
            "Epoch [196/500], Loss: 5.2010\n",
            "Epoch [197/500], Loss: 4.9846\n",
            "Epoch [198/500], Loss: 4.5558\n",
            "Epoch [199/500], Loss: 4.4975\n",
            "Epoch [200/500], Loss: 3.5821\n",
            "Epoch [201/500], Loss: 4.2605\n",
            "Epoch [202/500], Loss: 4.2080\n",
            "Epoch [203/500], Loss: 3.7203\n",
            "Epoch [204/500], Loss: 4.7024\n",
            "Epoch [205/500], Loss: 4.5011\n",
            "Epoch [206/500], Loss: 4.5598\n",
            "Epoch [207/500], Loss: 3.8283\n",
            "Epoch [208/500], Loss: 4.3120\n",
            "Epoch [209/500], Loss: 4.0300\n",
            "Epoch [210/500], Loss: 3.7270\n",
            "Epoch [211/500], Loss: 4.5572\n",
            "Epoch [212/500], Loss: 4.4426\n",
            "Epoch [213/500], Loss: 4.8266\n",
            "Epoch [214/500], Loss: 3.7875\n",
            "Epoch [215/500], Loss: 3.7149\n",
            "Epoch [216/500], Loss: 3.0030\n",
            "Epoch [217/500], Loss: 3.6612\n",
            "Epoch [218/500], Loss: 3.2715\n",
            "Epoch [219/500], Loss: 3.6287\n",
            "Epoch [220/500], Loss: 3.6107\n",
            "Epoch [221/500], Loss: 4.7132\n",
            "Epoch [222/500], Loss: 4.2972\n",
            "Epoch [223/500], Loss: 3.4484\n",
            "Epoch [224/500], Loss: 3.9743\n",
            "Epoch [225/500], Loss: 4.2063\n",
            "Epoch [226/500], Loss: 5.3831\n",
            "Epoch [227/500], Loss: 3.6900\n",
            "Epoch [228/500], Loss: 4.4577\n",
            "Epoch [229/500], Loss: 3.7551\n",
            "Epoch [230/500], Loss: 4.1138\n",
            "Epoch [231/500], Loss: 4.4335\n",
            "Epoch [232/500], Loss: 3.3183\n",
            "Epoch [233/500], Loss: 4.6246\n",
            "Epoch [234/500], Loss: 3.3534\n",
            "Epoch [235/500], Loss: 3.7421\n",
            "Epoch [236/500], Loss: 3.8329\n",
            "Epoch [237/500], Loss: 3.7260\n",
            "Epoch [238/500], Loss: 3.6397\n",
            "Epoch [239/500], Loss: 3.8669\n",
            "Epoch [240/500], Loss: 4.5090\n",
            "Epoch [241/500], Loss: 4.4896\n",
            "Epoch [242/500], Loss: 5.3117\n",
            "Epoch [243/500], Loss: 3.5838\n",
            "Epoch [244/500], Loss: 3.9488\n",
            "Epoch [245/500], Loss: 3.4788\n",
            "Epoch [246/500], Loss: 3.3745\n",
            "Epoch [247/500], Loss: 4.5605\n",
            "Epoch [248/500], Loss: 3.7758\n",
            "Epoch [249/500], Loss: 4.9010\n",
            "Epoch [250/500], Loss: 3.5336\n",
            "Epoch [251/500], Loss: 4.2766\n",
            "Epoch [252/500], Loss: 3.4238\n",
            "Epoch [253/500], Loss: 3.8036\n",
            "Epoch [254/500], Loss: 3.4208\n",
            "Epoch [255/500], Loss: 5.1705\n",
            "Epoch [256/500], Loss: 2.9193\n",
            "Epoch [257/500], Loss: 3.3049\n",
            "Epoch [258/500], Loss: 4.7895\n",
            "Epoch [259/500], Loss: 4.0092\n",
            "Epoch [260/500], Loss: 3.8455\n",
            "Epoch [261/500], Loss: 2.7329\n",
            "Epoch [262/500], Loss: 4.1469\n",
            "Epoch [263/500], Loss: 4.9904\n",
            "Epoch [264/500], Loss: 3.9706\n",
            "Epoch [265/500], Loss: 4.4808\n",
            "Epoch [266/500], Loss: 4.6428\n",
            "Epoch [267/500], Loss: 3.9976\n",
            "Epoch [268/500], Loss: 4.1214\n",
            "Epoch [269/500], Loss: 3.5966\n",
            "Epoch [270/500], Loss: 4.9436\n",
            "Epoch [271/500], Loss: 4.5019\n",
            "Epoch [272/500], Loss: 4.8977\n",
            "Epoch [273/500], Loss: 4.1927\n",
            "Epoch [274/500], Loss: 4.2266\n",
            "Epoch [275/500], Loss: 4.1510\n",
            "Epoch [276/500], Loss: 3.6820\n",
            "Epoch [277/500], Loss: 4.0369\n",
            "Epoch [278/500], Loss: 4.3029\n",
            "Epoch [279/500], Loss: 4.5812\n",
            "Epoch [280/500], Loss: 4.8219\n",
            "Epoch [281/500], Loss: 3.0513\n",
            "Epoch [282/500], Loss: 3.9508\n",
            "Epoch [283/500], Loss: 3.9538\n",
            "Epoch [284/500], Loss: 4.3523\n",
            "Epoch [285/500], Loss: 3.7841\n",
            "Epoch [286/500], Loss: 4.5986\n",
            "Epoch [287/500], Loss: 4.5117\n",
            "Epoch [288/500], Loss: 4.1386\n",
            "Epoch [289/500], Loss: 4.4566\n",
            "Epoch [290/500], Loss: 3.0039\n",
            "Epoch [291/500], Loss: 5.1501\n",
            "Epoch [292/500], Loss: 4.7816\n",
            "Epoch [293/500], Loss: 4.6237\n",
            "Epoch [294/500], Loss: 4.3576\n",
            "Epoch [295/500], Loss: 4.4438\n",
            "Epoch [296/500], Loss: 2.4669\n",
            "Epoch [297/500], Loss: 3.1530\n",
            "Epoch [298/500], Loss: 3.4607\n",
            "Epoch [299/500], Loss: 3.6317\n",
            "Epoch [300/500], Loss: 3.7952\n",
            "Epoch [301/500], Loss: 3.8057\n",
            "Epoch [302/500], Loss: 2.8635\n",
            "Epoch [303/500], Loss: 3.1022\n",
            "Epoch [304/500], Loss: 3.4304\n",
            "Epoch [305/500], Loss: 3.9934\n",
            "Epoch [306/500], Loss: 2.7261\n",
            "Epoch [307/500], Loss: 5.0625\n",
            "Epoch [308/500], Loss: 3.9502\n",
            "Epoch [309/500], Loss: 4.6618\n",
            "Epoch [310/500], Loss: 5.4665\n",
            "Epoch [311/500], Loss: 3.8817\n",
            "Epoch [312/500], Loss: 4.6535\n",
            "Epoch [313/500], Loss: 4.4951\n",
            "Epoch [314/500], Loss: 3.3182\n",
            "Epoch [315/500], Loss: 4.7520\n",
            "Epoch [316/500], Loss: 4.3696\n",
            "Epoch [317/500], Loss: 3.5806\n",
            "Epoch [318/500], Loss: 3.3485\n",
            "Epoch [319/500], Loss: 3.3573\n",
            "Epoch [320/500], Loss: 4.0324\n",
            "Epoch [321/500], Loss: 3.6801\n",
            "Epoch [322/500], Loss: 4.3223\n",
            "Epoch [323/500], Loss: 3.0623\n",
            "Epoch [324/500], Loss: 4.6417\n",
            "Epoch [325/500], Loss: 3.4147\n",
            "Epoch [326/500], Loss: 3.9409\n",
            "Epoch [327/500], Loss: 4.8455\n",
            "Epoch [328/500], Loss: 3.8769\n",
            "Epoch [329/500], Loss: 4.7637\n",
            "Epoch [330/500], Loss: 3.4862\n",
            "Epoch [331/500], Loss: 4.2769\n",
            "Epoch [332/500], Loss: 3.8217\n",
            "Epoch [333/500], Loss: 4.0014\n",
            "Epoch [334/500], Loss: 4.3643\n",
            "Epoch [335/500], Loss: 5.0837\n",
            "Epoch [336/500], Loss: 3.2085\n",
            "Epoch [337/500], Loss: 4.2647\n",
            "Epoch [338/500], Loss: 3.8609\n",
            "Epoch [339/500], Loss: 3.9465\n",
            "Epoch [340/500], Loss: 4.0033\n",
            "Epoch [341/500], Loss: 3.7388\n",
            "Epoch [342/500], Loss: 3.6286\n",
            "Epoch [343/500], Loss: 3.5813\n",
            "Epoch [344/500], Loss: 3.5381\n",
            "Epoch [345/500], Loss: 4.3364\n",
            "Epoch [346/500], Loss: 3.2244\n",
            "Epoch [347/500], Loss: 3.6867\n",
            "Epoch [348/500], Loss: 3.6386\n",
            "Epoch [349/500], Loss: 3.5653\n",
            "Epoch [350/500], Loss: 4.8495\n",
            "Epoch [351/500], Loss: 3.9270\n",
            "Epoch [352/500], Loss: 3.9544\n",
            "Epoch [353/500], Loss: 5.8434\n",
            "Epoch [354/500], Loss: 3.4971\n",
            "Epoch [355/500], Loss: 3.2522\n",
            "Epoch [356/500], Loss: 4.3867\n",
            "Epoch [357/500], Loss: 4.2213\n",
            "Epoch [358/500], Loss: 3.4133\n",
            "Epoch [359/500], Loss: 4.9497\n",
            "Epoch [360/500], Loss: 5.0058\n",
            "Epoch [361/500], Loss: 4.9909\n",
            "Epoch [362/500], Loss: 3.7502\n",
            "Epoch [363/500], Loss: 4.5946\n",
            "Epoch [364/500], Loss: 3.7977\n",
            "Epoch [365/500], Loss: 5.0944\n",
            "Epoch [366/500], Loss: 3.0866\n",
            "Epoch [367/500], Loss: 2.6182\n",
            "Epoch [368/500], Loss: 3.1599\n",
            "Epoch [369/500], Loss: 3.4642\n",
            "Epoch [370/500], Loss: 4.4397\n",
            "Epoch [371/500], Loss: 3.0171\n",
            "Epoch [372/500], Loss: 4.3913\n",
            "Epoch [373/500], Loss: 3.9093\n",
            "Epoch [374/500], Loss: 4.9882\n",
            "Epoch [375/500], Loss: 4.3701\n",
            "Epoch [376/500], Loss: 3.6976\n",
            "Epoch [377/500], Loss: 4.9526\n",
            "Epoch [378/500], Loss: 4.7669\n",
            "Epoch [379/500], Loss: 4.5255\n",
            "Epoch [380/500], Loss: 4.6056\n",
            "Epoch [381/500], Loss: 3.4773\n",
            "Epoch [382/500], Loss: 4.8385\n",
            "Epoch [383/500], Loss: 4.8135\n",
            "Epoch [384/500], Loss: 3.0002\n",
            "Epoch [385/500], Loss: 4.1563\n",
            "Epoch [386/500], Loss: 4.2772\n",
            "Epoch [387/500], Loss: 5.1293\n",
            "Epoch [388/500], Loss: 3.8157\n",
            "Epoch [389/500], Loss: 4.5018\n",
            "Epoch [390/500], Loss: 4.3853\n",
            "Epoch [391/500], Loss: 4.4778\n",
            "Epoch [392/500], Loss: 3.3744\n",
            "Epoch [393/500], Loss: 4.3357\n",
            "Epoch [394/500], Loss: 4.3688\n",
            "Epoch [395/500], Loss: 3.9088\n",
            "Epoch [396/500], Loss: 3.9489\n",
            "Epoch [397/500], Loss: 3.6558\n",
            "Epoch [398/500], Loss: 4.2983\n",
            "Epoch [399/500], Loss: 3.9398\n",
            "Epoch [400/500], Loss: 4.2330\n",
            "Epoch [401/500], Loss: 3.7919\n",
            "Epoch [402/500], Loss: 4.9409\n",
            "Epoch [403/500], Loss: 4.4285\n",
            "Epoch [404/500], Loss: 4.0550\n",
            "Epoch [405/500], Loss: 4.0077\n",
            "Epoch [406/500], Loss: 4.6997\n",
            "Epoch [407/500], Loss: 4.4620\n",
            "Epoch [408/500], Loss: 3.6250\n",
            "Epoch [409/500], Loss: 5.5446\n",
            "Epoch [410/500], Loss: 3.6278\n",
            "Epoch [411/500], Loss: 5.9160\n",
            "Epoch [412/500], Loss: 4.3785\n",
            "Epoch [413/500], Loss: 4.0685\n",
            "Epoch [414/500], Loss: 3.7619\n",
            "Epoch [415/500], Loss: 3.3803\n",
            "Epoch [416/500], Loss: 3.9928\n",
            "Epoch [417/500], Loss: 4.2143\n",
            "Epoch [418/500], Loss: 4.0090\n",
            "Epoch [419/500], Loss: 3.9352\n",
            "Epoch [420/500], Loss: 3.7057\n",
            "Epoch [421/500], Loss: 4.1523\n",
            "Epoch [422/500], Loss: 4.5511\n",
            "Epoch [423/500], Loss: 4.4517\n",
            "Epoch [424/500], Loss: 3.6929\n",
            "Epoch [425/500], Loss: 4.7932\n",
            "Epoch [426/500], Loss: 4.2397\n",
            "Epoch [427/500], Loss: 4.7346\n",
            "Epoch [428/500], Loss: 3.8088\n",
            "Epoch [429/500], Loss: 4.2772\n",
            "Epoch [430/500], Loss: 3.9580\n",
            "Epoch [431/500], Loss: 4.2381\n",
            "Epoch [432/500], Loss: 3.7216\n",
            "Epoch [433/500], Loss: 3.8395\n",
            "Epoch [434/500], Loss: 3.7288\n",
            "Epoch [435/500], Loss: 4.1520\n",
            "Epoch [436/500], Loss: 4.8729\n",
            "Epoch [437/500], Loss: 4.0454\n",
            "Epoch [438/500], Loss: 4.4577\n",
            "Epoch [439/500], Loss: 3.6923\n",
            "Epoch [440/500], Loss: 5.2806\n",
            "Epoch [441/500], Loss: 3.8675\n",
            "Epoch [442/500], Loss: 3.8180\n",
            "Epoch [443/500], Loss: 4.3509\n",
            "Epoch [444/500], Loss: 3.8216\n",
            "Epoch [445/500], Loss: 4.1701\n",
            "Epoch [446/500], Loss: 3.6373\n",
            "Epoch [447/500], Loss: 4.6693\n",
            "Epoch [448/500], Loss: 4.0359\n",
            "Epoch [449/500], Loss: 4.4971\n",
            "Epoch [450/500], Loss: 4.0234\n",
            "Epoch [451/500], Loss: 3.5863\n",
            "Epoch [452/500], Loss: 4.8946\n",
            "Epoch [453/500], Loss: 4.7015\n",
            "Epoch [454/500], Loss: 3.7937\n",
            "Epoch [455/500], Loss: 4.2435\n",
            "Epoch [456/500], Loss: 3.8759\n",
            "Epoch [457/500], Loss: 3.4595\n",
            "Epoch [458/500], Loss: 4.3918\n",
            "Epoch [459/500], Loss: 4.4666\n",
            "Epoch [460/500], Loss: 3.8944\n",
            "Epoch [461/500], Loss: 3.4207\n",
            "Epoch [462/500], Loss: 4.2412\n",
            "Epoch [463/500], Loss: 4.2782\n",
            "Epoch [464/500], Loss: 5.5307\n",
            "Epoch [465/500], Loss: 3.9475\n",
            "Epoch [466/500], Loss: 4.0497\n",
            "Epoch [467/500], Loss: 3.4986\n",
            "Epoch [468/500], Loss: 4.7176\n",
            "Epoch [469/500], Loss: 3.5516\n",
            "Epoch [470/500], Loss: 4.1270\n",
            "Epoch [471/500], Loss: 4.5367\n",
            "Epoch [472/500], Loss: 4.2846\n",
            "Epoch [473/500], Loss: 3.6957\n",
            "Epoch [474/500], Loss: 3.4450\n",
            "Epoch [475/500], Loss: 3.6937\n",
            "Epoch [476/500], Loss: 4.2024\n",
            "Epoch [477/500], Loss: 3.8670\n",
            "Epoch [478/500], Loss: 3.9940\n",
            "Epoch [479/500], Loss: 4.3064\n",
            "Epoch [480/500], Loss: 3.9118\n",
            "Epoch [481/500], Loss: 5.2044\n",
            "Epoch [482/500], Loss: 4.0147\n",
            "Epoch [483/500], Loss: 4.0776\n",
            "Epoch [484/500], Loss: 4.1666\n",
            "Epoch [485/500], Loss: 4.4556\n",
            "Epoch [486/500], Loss: 3.4545\n",
            "Epoch [487/500], Loss: 3.4324\n",
            "Epoch [488/500], Loss: 3.2128\n",
            "Epoch [489/500], Loss: 4.4132\n",
            "Epoch [490/500], Loss: 3.8500\n",
            "Epoch [491/500], Loss: 5.1080\n",
            "Epoch [492/500], Loss: 4.1468\n",
            "Epoch [493/500], Loss: 4.2068\n",
            "Epoch [494/500], Loss: 4.0484\n",
            "Epoch [495/500], Loss: 3.5721\n",
            "Epoch [496/500], Loss: 3.0992\n",
            "Epoch [497/500], Loss: 4.8508\n",
            "Epoch [498/500], Loss: 5.1573\n",
            "Epoch [499/500], Loss: 3.3167\n",
            "Epoch [500/500], Loss: 4.7213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"theta0 :\", mySimpleNet.linear.bias.data)\n",
        "print(\"theta 1 a 3 :\", mySimpleNet.linear.weight.data)\n",
        "# On retrouve bien des pods similaires"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ad67cd-2cd7-4722-c4c1-3404da012d01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta0 : tensor([15.5762])\n",
            "theta 1 a 3 : tensor([[3.0447, 3.0989, 6.2816]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tensor = torch.tensor(test_set['inputs']).float()\n",
        "predictions_nn = mySimpleNet(X_test_tensor).detach().numpy()\n",
        "\n",
        "mse_nn = mean_squared_error(test_set['targets'], predictions_nn)\n",
        "\n",
        "print(mse_nn)\n",
        "\n",
        "# On trouve les memes resultats, cela s'explique par le fait qu'un reseau de neurone a une couche et\n",
        "# sans fonction d'activation est bien linéaire et a donc les memes resultats que la regression lineaire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_xlKM_rVxlq",
        "outputId": "5ba27f74-eb8b-40a9-b50e-42692a740ce8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.195047326616335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1db519-86ff-4d18-e8f8-72410c06fe8d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "# Down1 : 64 -> 128\n",
        "# MaxPool1D(size = 2) 0\n",
        "# Double_conv_casual avec in_ch = 64 et out_ch = 128\n",
        "# batchnorm1d 2*n et relu 0\n",
        "# Conv1 = 64*3*128 + 128\n",
        "# Conv2 = 128*3*128 + 128\n",
        "# Total Down1\n",
        "# (64*3*128 + 128) + 2*128 + (128*3*128 + 128) + 2*128 = 74 496\n",
        "\n",
        "# Nb de paramètres au total:\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Nombre total de paramètres: {total_params}\")"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27685fa-7d2f-4473-e541-b0546c147fd6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de paramètres: 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La taille est reduite avec les maxpooling de down causal\n",
        "# Elle est restituée avec ConvTranspose et cat dans Up_causal"
      ],
      "metadata": {
        "id": "7SqMxVStZKj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Le champ réceptif est augmenté par des convolutions causales empilées.\n",
        "#(kernel_size - 1) * nb_convolutions + 1 = (3 - 1) * 2 + 1 = 5.\n",
        "# self.inc a un champ réceptif de 5"
      ],
      "metadata": {
        "id": "2i_FWGiQZt2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor2 = input_tensor1.clone()\n",
        "input_tensor2[0, 0, 0] = -input_tensor1[0, 0, 0]\n",
        "\n",
        "input_tensor2 = input_tensor1.clone()\n",
        "input_tensor2[0, 0, 0] = 1  # Modifier une composante\n",
        "\n",
        "print(input_tensor1[0,0,0])\n",
        "print(input_tensor2[0,0,0])\n",
        "\n",
        "# Obtenir les sorties du modèle pour les deux entrées\n",
        "output1 = model(input_tensor1)\n",
        "output2 = model(input_tensor2)\n",
        "\n",
        "# Trouver la taille du champ réceptif\n",
        "receptive_field_size = -1\n",
        "for i in range(len(output1[0, 0])):\n",
        "    if output1[0, 0, i] == output2[0, 0, i]:\n",
        "      receptive_field_size = i\n",
        "      break\n",
        "\n",
        "print(\"Taille du champ réceptif :\", receptive_field_size)\n"
      ],
      "metadata": {
        "id": "69WMWCSZAg5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5739a41-2915-4e43-e86a-5f3fa1759cb2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2244)\n",
            "tensor(1.)\n",
            "Taille du champ réceptif : 5916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeooRYE-ATGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?token=GHSAT0AAAAAAC427DACOPGNDNN6UDOLVLLAZ4BB2JQ\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}